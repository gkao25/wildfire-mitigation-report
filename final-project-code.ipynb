{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 180A Quarter 1 Project Code\n",
    "\n",
    "Wildfile Mitigation\n",
    "\n",
    "By: Gloria Kao, Shentong Li\n",
    "\n",
    "Outputs (tables, aggregated data, graphs, etc.) are commented out and not shown because of NDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. EDA\n",
    "\n",
    "EDA and merging of weather station datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# pacakges for geospatial analysis and plotting\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 5 datasets in total. We focus on 3 of them first: \n",
    "\n",
    "1. `gis_weatherstation_shape_2024_10_04.csv`: Information of weather stations such as names, location, structure details, etc.\n",
    "2. `src_wings_meteorology_station_summary_snapshot_2023_08_02.csv`: Meteorology data for each weather stations such as max gust and alert windspeed. \n",
    "3. `src_wings_meteorology_windspeed_snapshot_2023_08_02.csv`: Windspeed snapshots collected from weather stations, ranging from years 2012 to 2022. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gis_2024_1004 = pd.read_csv('data/gis_weatherstation_shape_2024_10_04.csv')\n",
    "station_summary_2023_08_02 = pd.read_csv('data/src_wings_meteorology_station_summary_snapshot_2023_08_02.csv')\n",
    "windspeed_2023_08_02 = pd.read_csv('data/src_wings_meteorology_windspeed_snapshot_2023_08_02.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Table 1 - GIS 2024_10_04\n",
    "#### 1.1.1 Basic Summary Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gis_2024_1004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gis_2024_1004.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_location = gis_2024_1004[['weatherstationcode', 'latitude', 'longitude']]\n",
    "station_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gis_2024_1004.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null\n",
    "gis_2024_1004.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num of stations contained\n",
    "gis_2024_1004['weatherstationname'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_stations = gis_2024_1004[gis_2024_1004.duplicated(subset=['weatherstationname'], keep=False)]\n",
    "duplicate_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of each values in 'nwszone'\n",
    "gis_2024_1004['nwszone'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Geospatial Analysis\n",
    "Show the details of each station by clicking on the icon in the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_center = [gis_2024_1004['latitude'].mean(), gis_2024_1004['longitude'].mean()]\n",
    "m1 = folium.Map(location=map_center, zoom_start=10)\n",
    "\n",
    "# Add weather station points to the map\n",
    "for _, row in gis_2024_1004.iterrows():\n",
    "    # Create a popup with relevant information\n",
    "    popup_text = f\"\"\"\n",
    "    Weather Station: {row['weatherstationname']}<br>\n",
    "    Elevation: {row['elevation']} m<br>\n",
    "    NWS Zone: {row['nwszone']}<br>\n",
    "    Structure ID: {row['structureid']}<br>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add a marker for each weather station\n",
    "    folium.Marker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        popup=folium.Popup(popup_text, max_width=300),\n",
    "        icon=folium.Icon(color='blue', icon='info-sign')\n",
    "    ).add_to(m1)\n",
    "\n",
    "\n",
    "boundary_coords = [\n",
    "    (gis_2024_1004['latitude'].min(), gis_2024_1004['longitude'].min()),\n",
    "    (gis_2024_1004['latitude'].min(), gis_2024_1004['longitude'].max()),\n",
    "    (gis_2024_1004['latitude'].max(), gis_2024_1004['longitude'].max()),\n",
    "    (gis_2024_1004['latitude'].max(), gis_2024_1004['longitude'].min())\n",
    "]\n",
    "\n",
    "# boundary box\n",
    "# folium.Polygon(locations=boundary_coords, color='green', fill=True, fill_opacity=0.2).add_to(m1)\n",
    "\n",
    "# m1.save('weather_stations_with_area_map.html')\n",
    "m1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Table 2 - Station Summary 2023_08_02\n",
    "#### 1.2.1 Basic Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_summary_2023_08_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_summary_2023_08_02.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution graphs \n",
    "sns.histplot(station_summary_2023_08_02['max_gust'], bins=10, kde=True)\n",
    "plt.title('Distribution of Maximum Gusts')\n",
    "plt.xlabel('Max Gust (mph)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "# plt.close()\n",
    "\n",
    "\n",
    "sns.histplot(station_summary_2023_08_02['99th'], bins=10, kde=True)\n",
    "plt.title('Distribution of 99th Percentile Gusts')\n",
    "plt.xlabel('99th Percentile Gust (mph)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "# plt.close()\n",
    "\n",
    "\n",
    "sns.histplot(station_summary_2023_08_02['95th'], bins=10, kde=True)\n",
    "plt.title('Distribution of 95th Percentile Gusts')\n",
    "plt.xlabel('99th Percentile Gust (mph)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='vri', data=station_summary_2023_08_02)\n",
    "plt.title('VRI (Risk Classification) Distribution')\n",
    "plt.xlabel('VRI (H = High, M = Medium, L = Low)')\n",
    "plt.ylabel('Count of Stations')\n",
    "plt.show()\n",
    "# plt.close()\n",
    "\n",
    "# Bar plot for Alert Levels\n",
    "sns.countplot(x='alert', data=station_summary_2023_08_02)\n",
    "plt.title('Alert Level Distribution')\n",
    "plt.xlabel('Alert Level')\n",
    "plt.ylabel('Count of Stations')\n",
    "plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the two datasets about weather stations together\n",
    "merged_df = pd.merge(station_summary_2023_08_02, gis_2024_1004, right_on= 'weatherstationcode', left_on='station', how='left')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = range(0, 5800, 400)  \n",
    "labels = [f'Group{i+1}: {bins[i]}-{bins[i+1]}' for i in range(len(bins)-1)]  # Create group labels\n",
    "\n",
    "# Assign the binned elevation groups\n",
    "merged_df['elevation_group'] = pd.cut(merged_df['elevation'], bins=bins, labels=labels)\n",
    "\n",
    "\n",
    "elevation_vri_grouped = merged_df.groupby('elevation_group')['vri'].value_counts().unstack().fillna(0)\n",
    "elevation_vri_grouped.plot(kind='bar', stacked=True, cmap='viridis')\n",
    "plt.title('VRI (Risk Classification) Across Elevation Groups', fontsize=14)\n",
    "plt.xlabel('Elevation Groups', fontsize=12)\n",
    "plt.ylabel('Number of Stations', fontsize=12)\n",
    "plt.legend(title='VRI Levels', loc='upper right')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vri_weights = {'H': 3, 'M': 2, 'L': 1}\n",
    "merged_df['vri_weight'] = merged_df['vri'].map(vri_weights)\n",
    "\n",
    "# Check for missing values and remove rows with NaN in latitude, longitude, or vri_weight\n",
    "cleaned_df = merged_df.dropna(subset=['latitude', 'longitude', 'vri_weight'])\n",
    "\n",
    "# Create a list of [latitude, longitude, weight] for the heatmap\n",
    "heat_data = [[row['latitude'], row['longitude'], row['vri_weight']] for index, row in cleaned_df.iterrows()]\n",
    "\n",
    "# Create a folium map centered around the average coordinates of the data\n",
    "m = folium.Map(location=[cleaned_df['latitude'].mean(), cleaned_df['longitude'].mean()], zoom_start=10)\n",
    "\n",
    "# Add the heatmap layer\n",
    "HeatMap(heat_data, min_opacity=0.2, radius=20, blur=15, max_zoom=1).add_to(m)\n",
    "\n",
    "# Save the map to an HTML file and display it\n",
    "# m.save('geospatial_risk_heatmap.html')\n",
    "\n",
    "# If running in Jupyter or similar environments, you can display the map directly\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Table 3 - Windspeed 2023_08_02\n",
    "#### 1.3.1 Basic Summary Stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windspeed_2023_08_02_edit = windspeed_2023_08_02.reset_index().drop(columns=['index'])\n",
    "windspeed_2023_08_02_edit['date'] = pd.to_datetime(windspeed_2023_08_02_edit['date'], format='%m/%d/%Y')\n",
    "windspeed_2023_08_02_edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_summary = windspeed_2023_08_02_edit.groupby('station')['wind_speed'].describe()\n",
    "station_summary_edit = station_summary.reset_index()\n",
    "station_summary_edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_wind_speed_merge = pd.merge(station_location, station_summary, left_on='weatherstationcode', right_on='station', how='right')\n",
    "location_wind_speed_merge_edit = location_wind_speed_merge.drop(columns=['weatherstationcode'])\n",
    "location_wind_speed_merge_edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = location_wind_speed_merge_edit.corr()\n",
    "sns.heatmap(matrix, cmap=\"Greens\", annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Seems that there is a correlation between the wind speed and the longitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windspeed_2023_08_02_group = windspeed_2023_08_02.groupby('station')['wind_speed'].mean()\n",
    "windspeed_2023_08_02_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for wind speed distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(windspeed_2023_08_02_edit['wind_speed'], bins=20, kde=True)\n",
    "plt.title('Distribution of Wind Speeds')\n",
    "plt.xlabel('Wind Speed (mph)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We have an outlier of windspeed over 600mph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windspeed_2023_08_02[windspeed_2023_08_02['wind_speed'] > 600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='station', y='wind_speed', data=windspeed_2023_08_02_edit)\n",
    "plt.title('Wind Speed Distribution by Station')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Station')\n",
    "plt.ylabel('Wind Speed (mph)')\n",
    "plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Analysis of windspeed over time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windspeed_2023_08_02_edit['month'] = windspeed_2023_08_02_edit['date'].dt.month\n",
    "\n",
    "month_summary = windspeed_2023_08_02_edit.groupby('month')['wind_speed'].describe()\n",
    "month_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "sns.lineplot(x='date', y='wind_speed', data=windspeed_2023_08_02_edit)\n",
    "plt.title('Wind Speed Over Time (All Stations)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Wind Speed (mph)')\n",
    "plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windspeed_2023_08_02_edit['date'] = pd.to_datetime(windspeed_2023_08_02_edit['date'])\n",
    "\n",
    "# Extract month and year from the date\n",
    "windspeed_2023_08_02_edit['month'] = windspeed_2023_08_02_edit['date'].dt.month\n",
    "windspeed_2023_08_02_edit['year'] = windspeed_2023_08_02_edit['date'].dt.year\n",
    "\n",
    "# Boxplot to show wind speed by month\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='month', y='wind_speed', data=windspeed_2023_08_02_edit)\n",
    "plt.title('Wind Speed by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Wind Speed (mph)')\n",
    "plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_corr = windspeed_2023_08_02_edit.groupby('month')['wind_speed'].mean()\n",
    "\n",
    "# Plot the average wind speed for each month\n",
    "plt.figure(figsize=(12, 6))\n",
    "seasonal_corr.plot(kind='bar')\n",
    "plt.title('Average Wind Speed by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Wind Speed (mph)')\n",
    "plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Probability of Weather Stations\n",
    "\n",
    "Calculating PSPS Probability of weather stations and displaying results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not all stations have the same number of windspeed records\n",
    "windspeed_grouped_count = windspeed_2023_08_02.groupby(by='station').count()\n",
    "windspeed_grouped_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_codes = np.array(gis_2024_1004['weatherstationcode'])\n",
    "merged_station_df = gis_2024_1004.merge(station_summary_2023_08_02, left_on='weatherstationcode', right_on='station', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: showing the windspeed alert threshold for the station \"AMO\"\n",
    "merged_df[merged_df['weatherstationcode']=='AMO']['alert'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the PSPS probabilities of all weather stations\n",
    "prob_lst = []\n",
    "\n",
    "for station in station_codes:\n",
    "    station_windspeeds = np.array(windspeed_2023_08_02[windspeed_2023_08_02['station'] == station]['wind_speed'])\n",
    "    # \"alert\" might be nan because of less entries in station_ss_df \n",
    "    has_threshold = True\n",
    "    try: \n",
    "        threshold = merged_df[merged_df['weatherstationcode'] == station]['alert'].iloc[0]\n",
    "    except:\n",
    "        has_threshold = False\n",
    "        prob = np.nan\n",
    "    mean = np.nanmean(station_windspeeds)\n",
    "    if has_threshold:\n",
    "        prob = np.mean([1 if x >= threshold else 0 for x in station_windspeeds]) * 100\n",
    "    count = np.count_nonzero(~np.isnan(station_windspeeds))\n",
    "    prob_lst.append([station, station_windspeeds, threshold, count, mean, prob])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing the probabilities as a dataframe\n",
    "prob_df = pd.DataFrame(prob_lst)\n",
    "prob_df.columns = ['station', 'windspeeds', 'threshold', 'count', 'mean', 'probability (%)']\n",
    "prob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('max prob: ' + str(prob_df['probability (%)'].max()))\n",
    "print('min prob: ' + str(prob_df['probability (%)'].min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# station mismatches between table 1 and table 2\n",
    "prob_mismatch = prob_df[prob_df['count'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort probability high to low\n",
    "prob_sorted = prob_df.sort_values(by='probability (%)', ascending=False)[:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stations with less than 50 windspeed records\n",
    "prob_less50 = prob_df[prob_df['count'] <50].sort_values(by='count', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_boxplot(station):\n",
    "    plt.figure(figsize =(4, 4))\n",
    "    subset = np.array(windspeed_2023_08_02[windspeed_2023_08_02['station'] == station]['wind_speed'])\n",
    "    sns.boxplot(subset, width=0.2)\n",
    "    threshold = prob_df[prob_df['station'] == station]['threshold'].iloc[0]\n",
    "    plt.axhline(threshold)\n",
    "    prob = prob_df[prob_df['station'] == station]['probability (%)'].iloc[0]\n",
    "    plt.text(x=0, y=38, s=f'probability: ' + str(prob), color='red')\n",
    "    plt.title(station)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can run a loop to show all stations distribtuion\n",
    "# for station in station_codes:\n",
    "#     dist_boxplot(station)\n",
    "\n",
    "# showing an example for station \"AMO\"\n",
    "dist_boxplot(\"AMO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Geospatial Visualization\n",
    "\n",
    "Merging weather station data to conductor spans and displaying PSPS Probabilities across all 3 layers geospatially\n",
    "\n",
    "### 3.1 New datasets\n",
    "\n",
    "Here we introduce the 2 other datasets. They have GeoPandas \"shape\" attributes. \n",
    "\n",
    "4. `src_vri_snapshot_2024_03_20.csv`: Geospatial data and risk category for the Vegetation Risk Index (VRI) polygons.\n",
    "5. `dev_wings_agg_span_2024_01_01.csv`: Information of conductor spans such as location, structure details, associates weather station, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vri_df = pd.read_csv('data/src_vri_snapshot_2024_03_20.csv')\n",
    "span_df = pd.read_csv('data/dev_wings_agg_span_2024_01_01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vri_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vri_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Changing shape columns to geometry type\n",
    "\n",
    "Currently, the `shape` column datatype is `str` when it should be geometry\n",
    "\n",
    "Also need to reproject to the same `shape_srid` ESPG:4326"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing 'shape' columns to gemoetry type and setting CRS to ESPG 4326\n",
    "\n",
    "gis_2024_1004['shape'] = gpd.GeoSeries.from_wkt(gis_2024_1004['shape'])\n",
    "gis_gdf = gpd.GeoDataFrame(gis_2024_1004, geometry='shape').set_crs(epsg=4431).to_crs(epsg=4326)\n",
    "\n",
    "vri_df['shape'] = gpd.GeoSeries.from_wkt(vri_df['shape'])\n",
    "vri_gdf = gpd.GeoDataFrame(vri_df, geometry='shape').set_crs(epsg=4326)\n",
    "\n",
    "span_df['shape'] = gpd.GeoSeries.from_wkt(span_df['shape'])\n",
    "span_gdf = gpd.GeoDataFrame(span_df, geometry='shape').set_crs(epsg=2230).to_crs(epsg=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the shape_srid columns since we have reprojected and they are no longer correct/meaningful \n",
    "gis_gdf = gis_gdf.drop(columns=['shape_srid'])\n",
    "vri_gdf = vri_gdf.drop(columns=['shape_srid'])\n",
    "span_gdf = span_gdf.drop(columns=['shape_srid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Merging datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge on weather station codes, not yet spatial join using gpd\n",
    "gis_vri_merge = gis_gdf.merge(vri_gdf, left_on='weatherstationcode', right_on='anemometercode')\n",
    "gis_vri_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find polygon centroids then merge with points\n",
    "vri_gdf['centroid'] = vri_gdf['shape'].centroid\n",
    "vri_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial join\n",
    "vri_gis_sjoin = vri_gdf.sjoin(gis_gdf, how='inner')\n",
    "vri_gis_sjoin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Found some anomolies with the dataframe sizes, there seem to be duplicates with the same station name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gis_gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vri_gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vri_gis_sjoin.shape\n",
    "# one extra row?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vri_gis_sjoin.index.nunique()\n",
    "# duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another spatial join\n",
    "vri_wingspan_sjoin = vri_gdf.sjoin(span_gdf)\n",
    "vri_wingspan_sjoin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vri_wingspan_sjoin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_gdf.shape\n",
    "# significantly less rows (intersections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualization with probabilities\n",
    "\n",
    "#### 3.2.1 Folium map with different layers\n",
    "\n",
    "Weather station markers, VRI risks (heatmap), VRI areas (polygons), PSPS probability (heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge prob_df with the new spatially joined df\n",
    "prob_merge = vri_gis_sjoin.merge(prob_df, left_on='weatherstationcode', right_on='station').merge(station_summary_2023_08_02, left_on='weatherstationcode', right_on='station')\n",
    "prob_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VRI risk heatmap\n",
    "vri_weights = {'H': 3, 'M': 2, 'L': 1}\n",
    "prob_merge['vri_weight'] = prob_merge['vri'].map(vri_weights)\n",
    "\n",
    "## PSPS probability heatmap \n",
    "# FIXME: heatmap weights not displaying correctly\n",
    "prob_quantiles = prob_merge['probability (%)'].quantile([0.25, 0.5, 0.75]).tolist()\n",
    "prob_weights = []\n",
    "for _, row in prob_merge.iterrows():\n",
    "    w = 0\n",
    "    if row['probability (%)'] < prob_quantiles[0]:\n",
    "        w = 1\n",
    "    elif row['probability (%)'] < prob_quantiles[1]:\n",
    "        w = 2\n",
    "    else:\n",
    "        w = 3\n",
    "    prob_weights.append(w)\n",
    "prob_merge['psps_weight'] = prob_weights\n",
    "\n",
    "# Check for missing values and remove rows with NaN in latitude, longitude, vri_weight, or probability\n",
    "cleaned_df = prob_merge.dropna(subset=['latitude', 'longitude', 'vri_weight', 'psps_weight'])\n",
    "\n",
    "# Create a list of [latitude, longitude, weight] for the heatmap\n",
    "heat_data = [[row['latitude'], row['longitude'], row['vri_weight']] for index, row in cleaned_df.iterrows()]\n",
    "\n",
    "# Create a folium map centered around the average coordinates of the data\n",
    "middle_point = [cleaned_df['latitude'].mean(), cleaned_df['longitude'].mean()]\n",
    "m = folium.Map(location=middle_point, zoom_start=10)\n",
    "\n",
    "# Add the heatmap layers\n",
    "heatmap_layer = folium.FeatureGroup(name='VRI risk')\n",
    "HeatMap(heat_data, min_opacity=0.2, radius=20, blur=15, max_zoom=1, name='VRI risk').add_to(heatmap_layer)\n",
    "heatmap_layer.add_to(m)\n",
    "\n",
    "psps_prob = folium.FeatureGroup(name='PSPS probability')\n",
    "heat_data2 = [[row['latitude'], row['longitude'], row['psps_weight']] for index, row in cleaned_df.iterrows()]\n",
    "HeatMap(heat_data2, min_opacity=0.2, radius=20, blur=15, max_zoom=1, name='PSPS prob').add_to(psps_prob)\n",
    "psps_prob.add_to(m)\n",
    "\n",
    "\n",
    "## Add weather station points to the map\n",
    "marker_group = folium.FeatureGroup(name=\"Weather stations\")\n",
    "for _, row in prob_merge.iterrows():\n",
    "    # Create a popup with relevant information\n",
    "    popup_text = f\"\"\"\n",
    "    Weather Station: {row['weatherstationname']} ({row['weatherstationcode']})<br>\n",
    "    Elevation: {row['elevation']} m<br>\n",
    "    NWS Zone: {row['nwszone']}<br>\n",
    "    PSPS Probability: {row['probability (%)']}<br>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add a marker for each weather station\n",
    "    folium.Marker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        popup=folium.Popup(popup_text, max_width=300),\n",
    "        icon=folium.Icon(color='blue', icon='info-sign')\n",
    "    ).add_to(marker_group)\n",
    "marker_group.add_to(m)\n",
    "\n",
    "\n",
    "## Add VRI polygons layer\n",
    "vri_polygons = folium.FeatureGroup(name='VRI polygons')\n",
    "for i in vri_gdf['shape']:\n",
    "    folium.GeoJson(i).add_to(vri_polygons)\n",
    "vri_polygons.add_to(m)\n",
    "\n",
    "\n",
    "# Create a layer control object and add it to our map instance\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# Save the map to an HTML file and display it\n",
    "# m.save('layered_map.html')\n",
    "\n",
    "# Display interactive map in Jupyter\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Conductor spans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_gdf.groupby(by='psps_station').count()\n",
    "# each psps station has a different number of conductor spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folium map object\n",
    "conductor_map = folium.Map(location=middle_point)\n",
    "\n",
    "## add weather station points to the map\n",
    "marker_group = folium.FeatureGroup(name=\"Weather stations\")\n",
    "for _, row in gis_gdf.iterrows():\n",
    "    # Create a popup with relevant information\n",
    "    popup_text = f\"\"\"\n",
    "    Weather Station: {row['weatherstationname']}<br>\n",
    "    Structure ID: {row['structureid']}<br>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add a marker for each weather station\n",
    "    folium.Marker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        popup=folium.Popup(popup_text, max_width=300),\n",
    "        icon=folium.Icon(color='blue', icon='info-sign')\n",
    "    ).add_to(marker_group)\n",
    "marker_group.add_to(conductor_map)\n",
    "\n",
    "# add hlines (each line is blue and very short)\n",
    "line_group = folium.FeatureGroup(name='Conductor spans')\n",
    "# only using the first 1000 lines as examples so the map/file isn't too large\n",
    "for i in span_gdf['shape'][:1000]:\n",
    "    folium.GeoJson(i).add_to(line_group)\n",
    "line_group.add_to(conductor_map)\n",
    "\n",
    "# add lines, grouped by the psps station it is tied to \n",
    "# commented out bc the full map/file becomes too large to be uploaded to github\n",
    "# for group_name, group_data in wingspan_gdf.groupby('psps_station'):\n",
    "#     feature_group = folium.FeatureGroup(name=str(group_name))\n",
    "#     for _, row in group_data.iterrows()[:1000]:\n",
    "#         folium.GeoJson(\n",
    "#             row['shape'],\n",
    "#             name=str(group_name)\n",
    "#         ).add_to(feature_group)\n",
    "#     feature_group.add_to(conductor_map)\n",
    "\n",
    "\n",
    "# add layer (to show the difference of added objects more clearly)\n",
    "folium.LayerControl().add_to(conductor_map)\n",
    "\n",
    "# Save the map to an HTML file and display it\n",
    "# conductor_map.save('conductor_span_map.html')\n",
    "\n",
    "conductor_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Network Graph\n",
    "\n",
    "Creating graph network object of spans to trace upstream/downtream the grid; collect list of weather stations that could cause a shut-off to any given span \n",
    "\n",
    "### 4.1 Making the Network Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engineering dataframe and types\n",
    "# making the \"globalid\" column str type\n",
    "\n",
    "span_gdf['globalid'] = span_gdf['globalid'].astype(str)\n",
    "span_gdf['globalid'] = [x[1:-1] for x in span_gdf['globalid']]\n",
    "\n",
    "span_gdf['upstream_span_id'] = span_gdf['upstream_span_id'].astype(str)\n",
    "span_gdf['upstream_span_id'] = [x[1:-1] for x in span_gdf['upstream_span_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upstream matches around half\n",
    "span_gdf['upstream_span_id'].isin(span_gdf['globalid']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty upstream \n",
    "span_gdf['seg_upstream_trace'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create graph\n",
    "down_g = nx.DiGraph()\n",
    "up_g = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add nodes\n",
    "down_g.add_nodes_from(span_gdf['globalid'])\n",
    "up_g.add_nodes_from(span_gdf['globalid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add node attributes\n",
    "partial_span_gdf = span_gdf[['globalid', 'upstream_span_id', 'seg_upstream_trace', 'psps_station']].reset_index()\n",
    "temp_merge = partial_span_gdf.merge(station_summary_2023_08_02, how='left', left_on='psps_station', right_on='station')\n",
    "partial_span_gdf = temp_merge[['globalid', 'upstream_span_id', 'seg_upstream_trace', 'psps_station', 'alert']].set_index('globalid')\n",
    "partial_span_dict = partial_span_gdf.to_dict('index')\n",
    "nx.set_node_attributes(down_g, partial_span_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match and add edges \n",
    "# .add_edge(u, v)  u -> v\n",
    "# direction should go from up to down stream so searching for impact will be easier\n",
    "down_edges = list(zip(span_gdf['upstream_span_id'], span_gdf['globalid']))\n",
    "down_g.add_edges_from(down_edges);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add edge downstream attributes\n",
    "up_edges = list(zip(span_gdf['globalid'], span_gdf['upstream_span_id']))\n",
    "up_g.add_edges_from(up_edges);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Graph Functions\n",
    "\n",
    "Functions that will be useful later, such as searching upstrea/downstream span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the network graph down_g\n",
    "\n",
    "def downstream_spans(span):\n",
    "    # uses bfs to find immediate downstream layer\n",
    "    # includes itself\n",
    "    search_edges = nx.bfs_edges(down_g, source=span)\n",
    "    downstream_nodes = [span] + [v for u, v in search_edges]\n",
    "    return downstream_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the network graph up_g\n",
    "\n",
    "def upstream_spans(span):\n",
    "    # uses bfs to find immediate downstream layer\n",
    "    # includes itself\n",
    "    search_edges = nx.bfs_edges(up_g, source=span)\n",
    "    upstream_nodes = [span] + [v for u, v in search_edges]\n",
    "    return upstream_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function test\n",
    "test_span = '23877069-F148-45BA-9EB8-CBE1DF9A6D87'\n",
    "print('The downstream spans of test span are:')\n",
    "print(downstream_spans(test_span))\n",
    "print('The upstream spans of test span are:')\n",
    "print(upstream_spans(test_span))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an attribute of a span\n",
    "def span_attribute(span, attr):\n",
    "    return down_g.nodes[span][attr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search which stations would need a psps given this windspeed\n",
    "def trigger_psps(windspeed):\n",
    "    yes_df = station_summary_2023_08_02[station_summary_2023_08_02['alert'] <= windspeed]\n",
    "    return list(yes_df['station'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search nodes with this attribute = key, and give its downstream spans\n",
    "# usage: you know the station name and wants to check the psps impact if that span is turned off\n",
    "\n",
    "def attr_search_node(attr, key):\n",
    "    attrs = nx.get_node_attributes(down_g, attr)\n",
    "    yes_nodes = []\n",
    "    for node, a in attrs.items():\n",
    "        if a == key:\n",
    "            yes_nodes.append(node)\n",
    "\n",
    "    downstream_list = []\n",
    "    for n in yes_nodes:\n",
    "        downstream_list.append(downstream_spans(n))\n",
    "    \n",
    "    output = pd.DataFrame(zip(yes_nodes, downstream_list))\n",
    "    output.columns = ['globalid', 'downstream_spans']\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Mapping Functions\n",
    "\n",
    "Functions that will help us visualize and validate our data. Creating a map can take minutes due to the number of spans, so test/example calls are not included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps all the spans associated with a given weather station code\n",
    "# takes a long time to run because of the large number of spans\n",
    "# example use: map_station_spans('VLC')\n",
    "\n",
    "def map_station_spans(station):\n",
    "    # create map\n",
    "    m = folium.Map(location=middle_point, zoom_start=10)\n",
    "\n",
    "    # add station marker to map\n",
    "    row = merged_df[merged_df['weatherstationcode'] == station]\n",
    "    # Create a popup with relevant information\n",
    "    popup_text = f\"\"\"\n",
    "    Weather Station: {row['weatherstationname'].iloc[0]}<br>\n",
    "    Structure ID: {row['structureid'].iloc[0]}<br>\n",
    "    Alert Windspeed: {row['alert'].iloc[0]}<br>\n",
    "    \"\"\"\n",
    "    folium.Marker(location=[row['latitude'], row['longitude']], \n",
    "                  popup=folium.Popup(popup_text, max_width=300),\n",
    "                  icon=folium.Icon(color='blue', icon='info-sign')).add_to(m)\n",
    "\n",
    "    # find related spans\n",
    "    spans_df = attr_search_node('psps_station', station)\n",
    "    spans_array = np.concatenate((spans_df['globalid'], spans_df['downstream_spans'].sum()))\n",
    "    spans_array = np.unique(spans_array)\n",
    "\n",
    "    # add spans to map\n",
    "    span_shapes = []\n",
    "    for i in spans_array:\n",
    "        span_shapes.append(span_gdf[span_gdf['globalid'] == i])\n",
    "    for i in span_shapes:\n",
    "        folium.GeoJson(i).add_to(m)\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps the downstream spans of a given span globalid\n",
    "# example use: map_downstream_spans('23877069-F148-45BA-9EB8-CBE1DF9A6D87')\n",
    "\n",
    "def map_downstream_spans(span):\n",
    "    # create map\n",
    "    m = folium.Map(location=middle_point, zoom_start=10)\n",
    "\n",
    "    # find downstream spans\n",
    "    spans_array = downstream_spans(span)\n",
    "\n",
    "    # add spans to map\n",
    "    span_shapes = []\n",
    "    for i in spans_array:\n",
    "        span_shapes.append(span_gdf[span_gdf['globalid'] == i]['shape'])\n",
    "    span_layer = folium.FeatureGroup(name='spans')\n",
    "    for i in span_shapes:\n",
    "        folium.GeoJson(i).add_to(span_layer)\n",
    "    span_layer.add_to(m)\n",
    "\n",
    "    # add marker because span might be too small\n",
    "    coords = span_shapes[0].iloc[0].coords\n",
    "    center = (coords[0][1], coords[0][0]) # lon lat is reversed\n",
    "    marker_layer = folium.FeatureGroup(name='span marker')\n",
    "    folium.Marker(location=center, icon=folium.Icon(color='red', icon='info-sign')).add_to(marker_layer)\n",
    "    marker_layer.add_to(m)\n",
    "\n",
    "    # returns multiple layers because this function is used inside another mapping function\n",
    "    return m, span_layer, marker_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps the upstream spans of a given span globalid\n",
    "# example use: map_upstream_spans('23877069-F148-45BA-9EB8-CBE1DF9A6D87')\n",
    "\n",
    "def map_upstream_spans(span):\n",
    "    # create map\n",
    "    m = folium.Map(location=middle_point, zoom_start=10)\n",
    "\n",
    "    # find downstream spans\n",
    "    spans_array = upstream_spans(span)\n",
    "\n",
    "    # add spans to map\n",
    "    span_shapes = []\n",
    "    for i in spans_array:\n",
    "        span_shapes.append(span_gdf[span_gdf['globalid'] == i]['shape'])\n",
    "    style = {'color': 'red'}\n",
    "    span_layer = folium.FeatureGroup(name='spans')\n",
    "    for i in span_shapes:\n",
    "        folium.GeoJson(i, style_function=lambda x:style).add_to(span_layer)\n",
    "    span_layer.add_to(m)\n",
    "\n",
    "    # add marker because span might be too small\n",
    "    coords = span_shapes[0].iloc[0].coords\n",
    "    center = (coords[0][1], coords[0][0]) # lon lat is reversed\n",
    "    marker_layer = folium.FeatureGroup(name='span marker')\n",
    "    folium.Marker(location=center, icon=folium.Icon(color='red', icon='info-sign')).add_to(marker_layer)\n",
    "    marker_layer.add_to(m)\n",
    "\n",
    "    # returns multiple layers because this function is used inside another mapping function\n",
    "    return m, span_layer, marker_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps the whole stream of a given span\n",
    "# example use: map_whole_stream('23877069-F148-45BA-9EB8-CBE1DF9A6D87')\n",
    "\n",
    "def map_whole_stream(span):\n",
    "    # create map\n",
    "    m = folium.Map(location=middle_point, zoom_start=10)\n",
    "\n",
    "    # combining the layers from the previous two functions into one \n",
    "    m1, down_layer, marker_layer = map_downstream_spans(span)\n",
    "    m2, up_layer, marker_layer = map_upstream_spans(span)\n",
    "\n",
    "    down_layer.add_to(m)\n",
    "    up_layer.add_to(m)\n",
    "    marker_layer.add_to(m)\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Probability of Conductor Spans\n",
    "\n",
    "Computing PSPS Probability of every conductor span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_station_psps_probability(windspeed_data, station_summary_data):\n",
    "    station_psps_probabilities = {}\n",
    "\n",
    "    for _, row in station_summary_data.iterrows():\n",
    "        station = row['station']\n",
    "        alert_speed = row['alert']\n",
    "\n",
    "        station_wind_data = windspeed_data[windspeed_data['station'] == station]\n",
    "        if station_wind_data.empty:\n",
    "            continue\n",
    "\n",
    "        count_above_alert = sum(station_wind_data['wind_speed'] >= alert_speed)\n",
    "        psps_prob = count_above_alert / len(station_wind_data)\n",
    "        station_psps_probabilities[station] = psps_prob\n",
    "\n",
    "    return station_psps_probabilities\n",
    "\n",
    "station_psps_probabilities = calculate_station_psps_probability(windspeed_2023_08_02, station_summary_2023_08_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first weather station psps\n",
    "first_key = next(iter(station_psps_probabilities))\n",
    "first_value = station_psps_probabilities[first_key]\n",
    "first_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_psps_prob_station = max(station_psps_probabilities, key=station_psps_probabilities.get)\n",
    "print(max_psps_prob_station, \":\",station_psps_probabilities[max_psps_prob_station])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight the contributions from downstream spans based on the distance to the given span. Total weight starts at 1 for the given span and then adds the weights of all downstream spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx.algorithms.shortest_paths.weighted as nx_shortest_path\n",
    "\n",
    "def calculate_psps_probability(conductor_span_data, windspeed_data, down_g, station_psps_probabilities):\n",
    "    psps_probabilities = {}\n",
    "\n",
    "    for _, row in conductor_span_data.iterrows():\n",
    "        span_id = row['globalid']\n",
    "\n",
    "        psps_station = row['psps_station']\n",
    "        if pd.isna(psps_station) or psps_station not in station_psps_probabilities:\n",
    "            continue\n",
    "\n",
    "        span_psps_prob = station_psps_probabilities[psps_station]\n",
    "\n",
    "        downstream_spans = list(down_g.successors(span_id)) if span_id in down_g else []\n",
    "        total_weight = 1.0  # initiate the weight for the given span\n",
    "        weighted_sum = span_psps_prob\n",
    "\n",
    "        for downstream_span in downstream_spans:\n",
    "            if downstream_span in psps_probabilities:\n",
    "                # distance between the given span and downstream span\n",
    "                try:\n",
    "                    distance = nx_shortest_path.dijkstra_path_length(down_g, span_id, downstream_span, weight='weight')\n",
    "                    weight = 1 / (distance + 1)  # weight is inversely proportional to distance\n",
    "                    total_weight += weight\n",
    "                    weighted_sum += psps_probabilities[downstream_span] * weight\n",
    "                except nx.NetworkXNoPath:\n",
    "                    continue\n",
    "\n",
    "        psps_prob = weighted_sum / total_weight\n",
    "        psps_probabilities[span_id] = psps_prob\n",
    "\n",
    "    return psps_probabilities\n",
    "\n",
    "# PSPS probabilities for all spans\n",
    "psps_probabilities = calculate_psps_probability(span_df, windspeed_2023_08_02, down_g, station_psps_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task1: Span(s) impacted by the greatest number of weather stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_counts = span_df['psps_station'].value_counts()\n",
    "max_station_count = station_counts.max()\n",
    "spans_with_max_stations = span_df[span_df['psps_station'].isin(station_counts[station_counts == max_station_count].index)]['globalid'].unique()\n",
    "print(spans_with_max_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Span with the highest probability of being shut off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_psps_prob_span = max(psps_probabilities, key=psps_probabilities.get)\n",
    "print(max_psps_prob_span, \":\",psps_probabilities[max_psps_prob_span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_id = max_psps_prob_span\n",
    "span_details = span_df[span_df['globalid'] == span_id]\n",
    "print(span_details['station'])\n",
    "downstream_spans = list(down_g.successors(span_id[1:-1]))\n",
    "has_downstream = len(downstream_spans) > 0\n",
    "print(has_downstream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Probability of any span within parent feederid 222 being shut off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same feederid 222 is recorded as both an int and str in the dataset\n",
    "# so we concatenate them into one dataset with 3024 spans\n",
    "df1 = span_df[span_df['parent_feederid'] == '222']\n",
    "df2 = span_df[span_df['parent_feederid'] == 222]\n",
    "feeder222_df = pd.concat([df1, df2], ignore_index=True)\n",
    "feeder222_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feeder_222_spans = feeder222_df['globalid']\n",
    "feeder_222_probs = [psps_probabilities[span] for span in feeder_222_spans if span in psps_probabilities]\n",
    "feeder_222_prob = sum(feeder_222_probs) / len(feeder_222_probs) if feeder_222_probs else 0\n",
    "feeder_222_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Estimating the Future\n",
    "\n",
    "Estimate expected customers that'll be shut-off over the next 10 years at the span/segment/circuit granularity.\n",
    "\n",
    "The code below focuses on feederid 222."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_customers_shutoff = 0\n",
    "for span in feeder_222_spans:\n",
    "    if span in psps_probabilities:\n",
    "        customer_count = span_df[span_df['globalid'] == span]['downstream_cust_total'].values\n",
    "        if len(customer_count) > 0:\n",
    "            expected_customers_shutoff += psps_probabilities[span] * customer_count[0]\n",
    "print(expected_customers_shutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
